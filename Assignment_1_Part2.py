# -*- coding: utf-8 -*-
"""Assignment_1_Part2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nuYyL7ar_6u4N7xXN7_2Wwe44xDHxSav
"""

# Initial necessary imports for data processing
# This includes imports for: loading and preprocessing the data, building the decision tree, visualizing the tree and checking for accuracy.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import tree
import graphviz
from sklearn.model_selection import cross_val_score

# We are loading our dataset here. 

df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', sep=',')
adult.shape

# As our dataset only contains records and not the names of columns, we need to do that here.

df.columns = [ 'age',
 'workclass',
 'fnlwgt',
 'education', 'education-num',
 'martial-status',
 'occupation',
 'relationship',
 'race',
 'sex',
   'capital-gain',
    'capital-loss',
    'hours-per-week',
    'native_Country', 'income']

df.head()

# Prints the first 5 rows

df.info()

# Delete null values within the dataset
# This null values might throw off our data so we need to get rid of them

df = df[(df.astype(str) != ' ?').all(axis=1)]
len(df)

# Here we have to create our output variable. Let us name it income_output
# We are going to assign either value 1 if income is greater than 50K or 0 if it is less/equalto 50K

df['income_output'] = df.apply(lambda row: 1 if '>50K'in row['income'] else 0, axis=1)

# Now, here, we can get rid of some columns that don't help us with out classification
# Columns like fnlwgt, capitl-gain, native_Country don't help us in determining income_output of an adult.

df = df.drop(['income','fnlwgt','capital-gain','capital-loss','native_Country'], axis=1)

# We need to encode the other columns so they have numerical values
# We can do this using get_dummies function

df = pd.get_dummies(df, columns=['workclass', 'education', 'martial-status', 'occupation', 'relationship', 'race', 'sex'])

df.head()
# Print to see our new table of data

df = df.sample(frac=1)

# Now we need to split our data into training and test sets
# Let us split first 25000 rows into train and rest into test. 
d_train = df[:25000]
d_test = df[25000:]

d_train_dat = d_train.drop(['income_output'], axis=1)
d_train_out = d_train['income_output']
d_test_dat = d_test.drop(['income_output'], axis=1)
d_test_out = d_test['income_output']
d_dat = df.drop(['income_output'], axis=1)
d_out = df['income_output']
# number of income > 50K in whole dataset:
print("Income >50K: %d out of %d (%.2f%%)" % (np.sum(d_gt50), len(d_gt50), 100*float(np.sum(d_gt50)) / len(d_gt50)))
# Income >50K: 7508 out of 30162 (24.89%)

# Fit a decision tree
# Entropy is measure of the randomness in the information being processed.
t = tree.DecisionTreeClassifier(criterion='entropy', max_depth=7)
t = t.fit(d_train_dat, d_train_out)

# Visualize tree
tree_visualize = tree.export_graphviz(t, out_file=None, label='all', impurity=False, proportion=True, 
                               feature_names=list(d_train_dat), class_names=['lt50K', 'gt50K'],
                               filled=True, rounded=True)
graph = graphviz.Source(tree_visualize)
graph

t.score(d_test_dat, d_test_out)
# We get around 82-83% accuracy which shows our model correctly predicts the income range of indivudals over 80% of the time.
# Overall, our decision tree classification class is quite good as it is able to predict income ranges based on numerous said features over 80% of the time.

# This shows the avarage score about two standard deviations away +/-
scores = cross_val_score(t, d_dat, d_out, cv=5)
print( 'Accuracy: %0.2f (+/- %0.2f)'% (scores.mean(), scores.std()*2))













